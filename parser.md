Парсер
	обход сайтов
		- список задан заранее
		
		обход страниц
			- список страниц/разделов задан заранее?
			- как узнавать о появлении новых страниц/разделов?

			обход товаров на странице
				- под каждый магазин, необходимо будет написать свой парсер товаров

	- все сайты отдают контент с сервера отрендеренный?
		- headless-chrome не нужен?
		(необходимо исследование)

	- SLA 10 секунд
		пусть на каждом сайте будет грубо 200 товаров
		200 * 26 = 5200 / 60 = 87 страниц в секунду - это довольно много
			- высокие требования к железу (затраты на железо в облаке)
			- потребуется набор прокси-серверов, поскольку такое количество краулеров будет лочиться антиддос и антиспам-системами
		- пункт 1 - обход всего сайта внутри одного потока - сильно сомневаюсь, что можно будет уложиться в текущий SLA
		- работа в чисто многопоточном режиме - будет крайне неэффективно, посольку основная работа парсера - сетевые взаимодействия
			- лучше проектировать все в асинхронном виде, так можно будет значительно лучше утилизировать железо
			ну и сам питон работает в многопоточном режиме крайне неэффективно из-за GIL

	- конвертер цен
		автоматическое получение курса валют

	- кэширование товаров - нужна база
		- сначала нужно уточнить требования
		- варианты mondo, postgres
		- учесть: полнотекстовый поиск
		- необходимо ли шардирование?
		- несколько ДЦ? работа в режиме ДЦ - 1?

	- переводы
		- автоматические переводы через yandex.translate и google.translate нормального качества не дадут, будет хороший материал для standUp-а
			а) нужны специализированные системы перевода
			б) на первом этапе можно использовать асессоров для верификации переводов, позднее можно будет обучить модель и заменить ею асессоров

	- autoscale системы
		(пункт 13)
			в принципе, если количество сайтов стабильно и сильно не меняется, количество страниц на них также сильно не изменяется,
			то нагрузка должна быть стабильной и особой необходимости в скэйлинге нет

	- система уведомлений
		- почта
		- телеграм

	- статистика
		- статистика проходов
		- по товарам
		- запросы

	- мониторинги - нужны уведомления о следующих ситуациях:
		- какие-то прокси будут блокироваться и переставать работать (атоматизируемо, подумать)
		- появление новых страниц/разделов
		- сайты могут менять свой дизайн и тогда страницы могут переставать парситься
		- сайты могут отдавать статусы с ошибками, в таких случаях надо будет разбираться


Админка
	- нужен дизайн страниц и разделов
	- поиск по товарам
	- просмотр информации о товаре
	- что еще?


devOps
	- разворачивание в облаке, мониторинги
	- статистика проходов парсинга
		- ELK - ?


Вопросы:
 - пункт 11 - это необходимо делать из админки?
 - надо ли выкачивать фотографии товаров?
 - нужна ли история изменений цены на данный товар?
 - пункт 8 - непонятно, что делать, если один и тот же товар встречается на разных страницах и в разных списках
 - пункт 10 - нетривиально
    а) на каждом сайте есть пример того, как выглядит скидка. Тогда можно добавить ее описание в парсер и парсер сможет ее детектить и слать уведомления
    б) система умеет строить анализ изменения цены на товар и в случае изменения цены более чем на n%, будет слать уведомления (возможно ложноположительные срабатывания)
    в) более сложная система с элементами машинного обучения, можно рассматривать скриншоты сайтов визуально - будут стоить очень дорого
 - пункт 13 - нужны пояснения
 - пункт 14 - как сопоставлять между собой товары с русс рынка с товарами с тур рынка? У них могут быть разные названия
 - пункт 15 - нужны пояснения
 - пункт 16 - нужны пояснения
 - в целом, такая система точно не будет работать так, что ее раз завел и она работает сама по себе. Система периодически будет сбоить, требовать обновлений, ...


=================================

Заключение
 - один человек за вменяемое время это точно не сделает
 - если это сделает один человек, вы получите нежизнеспособное решение, которое в последствии будет некому поддерживать и вам придется его либо выбросить, либо переписать заново
 - необходима команда
 - со старта большая команда, не нужна, лучше наращивать постепенно
 - срок разработки - о сроках говорить бессмысленно, т.к. нет ни дизайна, ни четкого ТЗ, ни команды, которая будет этим заниматься
 - методология разработки - scrum
   - двухнедельные релизы
   - в конце кажого спринта - демонстрация разработанного функционала в проде


План на начальные спринты:

1.
 - создание рабочих окружений
 - заведение репозиториев
 - линтеры, хуки, CI, ...
 - прокси
 - облако (можно обойтись на первых этапах)
(короткий спринт в 1 неделю)

2.
взять один сайт из списка и написать для него парсер
 - обходить страницы, товары, выгребать данные
работа над админкой - авторизация

3.
взять еще 2-3 сайта и повторить сбор данных на них
работа над админкой - будет зависеть от дизайна

4.
создание модели данных
выкачивание изображений
работа над админкой - будет зависеть от дизайна

5.
Создание системы распараллеливания парсинга в многопроцессном/многопоточном режиме
работа с прокси

6.
дописывание парсеров под оставшиеся сайты
рефакторинг
работа над админкой - будет зависеть от дизайна



